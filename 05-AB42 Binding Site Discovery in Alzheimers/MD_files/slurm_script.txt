#!/bin/bash
#SBATCH --time=168:00:00   # walltime
#SBATCH --ntasks=48  # number of processor cores (i.e. tasks)
#SBATCH --nodes=1   # number of nodes
#SBATCH --gres=gpu:0
#SBATCH --mem-per-cpu=1G   # memory per CPU core
#SBATCH --partition=expansion
#SBATCH -J "WT"   # job name
#SBATCH --qos=normal
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
#SBATCH --mail-user=slei2@caltech.edu

#load the modules from RHEL9
module load cuda
module load openmpi
module load plumed
module load gromacs/2022.3-gcc-11.3.1-7bjvc63
export OMP_NUM_THREADS=2

#srun gmx_mpi editconf -f hnrnpa1_cg_1 -o box.gro -c -d 1.0 -bt cubic
#srun gmx_mpi solvate -cp box.gro -cs water.gro -o solv.gro -p topol.top
#srun gmx_mpi grompp -f ions.mdp -c solv.gro -p topol.top -o ions.tpr
#srun gmx_mpi genion -s ions.tpr -o solv_ions.gro -p topol.top -pname NA -pname CL -netural -conc 0.15

srun gmx_mpi grompp -f em.mdp -c solv_ions.gro -n index.ndx -p topol.top -o em -maxwarn 5 #-ntomp $OMP_NUM_THREADS -d1b auto -v
srun gmx_mpi mdrun -deffnm em -ntomp $OMP_NUM_THREADS -dlb auto -v
srun gmx_mpi grompp -f nvt_1ns.mdp -c em.gro -r em.gro -n index.ndx -p topol.top -o nvt -maxwarn 5 #-ntomp $OMP_NUM_THREADS -d1b auto -v
srun gmx_mpi mdrun -deffnm nvt -ntomp $OMP_NUM_THREADS -dlb auto -v
srun gmx_mpi grompp -f npt_1ns.mdp -c nvt.gro -r nvt.gro -t nvt.cpt -n index.ndx -p topol.top -o npt -maxwarn 5 #-ntomp $OMP_NUM_THREADS -d1b auto -v
srun gmx_mpi mdrun -deffnm npt -ntomp $OMP_NUM_THREADS -dlb auto -v 
srun gmx_mpi grompp -f Restraint_20ns.mdp -c npt.gro -r npt.gro -t npt.cpt -n index.ndx -p topol.top -o md_1 -maxwarn 5 #-ntomp $OMP_NUM_THREADS -d1b auto -v
srun gmx_mpi mdrun -deffnm md_1 -ntomp $OMP_NUM_THREADS -dlb auto -v
srun gmx_mpi grompp -f prod_1ns.mdp -c md_1.gro  -t md_1.cpt -n index.ndx -p topol.top -o md_2 -maxwarn 5 #-ntomp $OMP_NUM_THREADS -d1b auto -v
srun rm *#*
srun gmx_mpi mdrun -deffnm md_2 -ntomp $OMP_NUM_THREADS -dlb auto -v 
#srun gmx_mpi mdrun -deffnm md_1ns -ntomp $OMP_NUM_THREADS -dlb auto -v -plumed plumed.dat
